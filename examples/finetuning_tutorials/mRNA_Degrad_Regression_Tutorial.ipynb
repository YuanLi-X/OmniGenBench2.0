{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "382bb623ad50000d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "OmniGenome is a comprehensive package designed for pretrained genomic foundation models (gFMs) development and benchmark.\n",
    "OmniGenome have the following key features:\n",
    "- Automated genomic FM benchmarking on public genomic datasets\n",
    "- Scalable genomic FM training and fine-tuning on genomic tasks\n",
    "- Diversified genomic FMs implementation\n",
    "- Easy-to-use pipeline for genomic FM development with no coding expertise required\n",
    "- Accessible OmniGenome Hub for sharing FMs, datasets, and pipelines\n",
    "- Extensive documentation and tutorials for genomic FM development\n",
    "\n",
    "This notebook provides a demonstration of OmniGenome's capabilities using the mRNA degradation regression task.\n"
   ],
   "id": "9118cec958f69f2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Requirements\n",
    "OmniGenome requires the following recommended dependencies:\n",
    "- Python 3.9+\n",
    "- PyTorch 2.0.0+\n",
    "- Transformers 4.37.0+\n",
    "- Pandas 1.3.3+\n",
    "- Others in case of specific tasks"
   ],
   "id": "109e0efa47756ae3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "!pip install -U omnigenbench  # Install OmniGenome package",
   "id": "7a4f9343bc0350cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning Genomic FMs on mRNA Degradation Regression Task\n",
    "\n",
    "mRNA degradation regression is a task that predicts the degradation rate of mRNA transcripts based on their sequences. The dataset is from the RGB benchmark, which contains mRNA sequences and their corresponding degradation rates. The task is to train a model that can accurately predict the degradation rate from the sequence."
   ],
   "id": "7a12cd82625ee12b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Import Libraries",
   "id": "1bce7dd5e0999f65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import autocuda\n",
    "import torch\n",
    "from metric_visualizer import MetricVisualizer\n",
    "\n",
    "from omnigenbench import OmniDatasetForTokenRegression  # Token regression means that the model predicts a continuous value for each token (e.g., nucleotide base) in the sequence.\n",
    "from omnigenbench import RegressionMetric\n",
    "from omnigenbench import OmniSingleNucleotideTokenizer, OmniKmersTokenizer\n",
    "from omnigenbench import OmniModelForTokenRegression\n",
    "from omnigenbench import Trainer"
   ],
   "id": "a4e37aa5ceefe10e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 2: Define and Initialize the Tokenizer",
   "id": "18039cd8bdd9824b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The is FM is exclusively powered by the OmniGenome package\n",
    "model_name_or_path = \"anonymous8/OmniGenome-52M\"\n",
    "\n",
    "# Generally, we use the tokenizers from transformers library, such as AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# However, OmniGenome provides specialized tokenizers for genomic data, such as single nucleotide tokenizer and k-mers tokenizer\n",
    "# we can force the tokenizer to be used in the model\n",
    "tokenizer = OmniSingleNucleotideTokenizer.from_pretrained(model_name_or_path)"
   ],
   "id": "eb3410dd88458683"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 3: Define and Initialize the Model",
   "id": "bc3f458cbdeac083"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We have implemented a diverse set of genomic models in OmniGenome, please refer to the documentation for more details\n",
    "reg_model = OmniModelForTokenRegression(\n",
    "    model_name_or_path,\n",
    "    tokenizer=tokenizer,\n",
    "    num_labels=3,\n",
    ")"
   ],
   "id": "601a6067622c6f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Define and Load the Dataset",
   "id": "67718155cb3bde05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "# necessary hyperparameters\n",
    "epochs = 10\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 1e-5\n",
    "batch_size = 8\n",
    "max_length = 128\n",
    "seeds = [45]  # Each seed will be used for one run\n",
    "\n",
    "\n",
    "# The Dataset class is a subclass of OmniDatasetForTokenRegression, which is designed for token regression tasks.\n",
    "\n",
    "class Dataset(OmniDatasetForTokenRegression):\n",
    "    def __init__(self, data_source, tokenizer, max_length, **kwargs):\n",
    "        super().__init__(data_source, tokenizer, max_length, **kwargs)\n",
    "\n",
    "    def prepare_input(self, instance, **kwargs):\n",
    "        target_cols = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\"]\n",
    "        instance[\"sequence\"] = f'{instance[\"sequence\"]}'\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            instance[\"sequence\"],\n",
    "            padding=kwargs.get(\"padding\", \"do_not_pad\"),\n",
    "            truncation=kwargs.get(\"truncation\", True),\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = [instance[target_col] for target_col in target_cols]\n",
    "        labels = np.concatenate(\n",
    "            [\n",
    "                np.array(labels),\n",
    "                np.array(\n",
    "                    [\n",
    "                        [-100]\n",
    "                        * (len(tokenized_inputs[\"input_ids\"].squeeze()) - len(labels[0])),\n",
    "                        [-100]\n",
    "                        * (len(tokenized_inputs[\"input_ids\"].squeeze()) - len(labels[0])),\n",
    "                        [-100]\n",
    "                        * (len(tokenized_inputs[\"input_ids\"].squeeze()) - len(labels[0])),\n",
    "                    ]\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        ).T\n",
    "        tokenized_inputs[\"labels\"] = torch.tensor(labels, dtype=torch.float32)\n",
    "        for col in tokenized_inputs:\n",
    "            tokenized_inputs[col] = tokenized_inputs[col].squeeze()\n",
    "        return tokenized_inputs\n",
    "\n",
    "# Load the dataset according to the path\n",
    "train_file = \"toy_datasets/RNA-mRNA/train.json\"\n",
    "test_file = \"toy_datasets/RNA-mRNA/test.json\"\n",
    "\n",
    "\n",
    "\n",
    "train_set = Dataset(\n",
    "    data_source=train_file,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    ")\n",
    "test_set = Dataset(\n",
    "    data_source=test_file,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)"
   ],
   "id": "77bb3c26423dadcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5: Define the Metrics\n",
    "We have implemented a diverse set of genomic metrics in OmniGenome, please refer to the documentation for more details.\n",
    "Users can also define their own metrics by inheriting the `OmniGenomeMetric` class.\n",
    "The `compute_metrics` can be a metric function list and each metric function should return a dictionary of metrics."
   ],
   "id": "31240209d1705e0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "compute_metrics = [\n",
    "    RegressionMetric(ignore_y=-100).root_mean_squared_error,\n",
    "    RegressionMetric(ignore_y=-100).r2_score,\n",
    "]\n"
   ],
   "id": "8f70cba9af5b9ca1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 6: Define and Initialize the Trainer",
   "id": "ced577fd79468ea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize the MetricVisualizer for logging the metrics\n",
    "\n",
    "for seed in seeds:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        reg_model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=reg_model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        optimizer=optimizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        seeds=seed,\n",
    "        device=autocuda.auto_cuda(),\n",
    "    )\n",
    "\n",
    "    metrics = trainer.train()\n",
    "    test_metrics = metrics[\"test\"]\n",
    "    print(metrics)\n"
   ],
   "id": "7b3009d3a4a082fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 7. Experimental Results Visualization\n",
    "The experimental results are visualized in the following plots. The plots show the F1 score and accuracy of the model on the test set for each run. The average F1 score and accuracy are also shown."
   ],
   "id": "12f33336b1cd1547"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 8. Model Checkpoint for Sharing\n",
    "The model checkpoint can be saved and shared with others for further use. The model checkpoint can be loaded using the following code:"
   ],
   "id": "c6ec39ba9f645f8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "path_to_save = \"OmniGenome-52M-mRNA\"\n",
    "reg_model.save(path_to_save, overwrite=True)\n",
    "\n",
    "# Load the model checkpoint\n",
    "reg_model = reg_model.load(path_to_save)\n",
    "results = reg_model.inference(\"CAGUGCCGAGGCCACGCGGAGAACGAUCGAGGGUACAGCACUA\")\n",
    "print(results[\"predictions\"])\n",
    "print(\"logits:\", results[\"logits\"])"
   ],
   "id": "724ecda60e901a6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 9. Ready-to-use Models from Fine-tuning\n",
    "All the models trained in this tutorial are available on the OmniGenome Hub, which is a Huggingface Spaces for sharing models, datasets, and pipelines. Users can easily access and use these models for their own tasks."
   ],
   "id": "d74afeac7d05d563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We can load the model checkpoint using the ModelHub\n",
    "from omnigenbench import ModelHub\n",
    "\n",
    "ssp_model = ModelHub.load(\"OmniGenome-52M-mRNA\")\n",
    "results = ssp_model.inference(\"CAGUGCCGAGGCCACGCGGAGAACGAUCGAGGGUACAGCACUA\")\n",
    "print(results[\"predictions\"])\n",
    "print(\"logits:\", results[\"logits\"])"
   ],
   "id": "aff30ee45d4b58ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
