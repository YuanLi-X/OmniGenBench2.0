{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aca3fd9",
   "metadata": {},
   "source": [
    "# Genomic Foundation Model Auto-Benchmarking\n",
    "This script is used to auto-benchmark the Genomic Foundation Model on diversified downstream tasks. \n",
    "We have automated the benchmark pipeline based on the OmniGenome package. \n",
    "Once your foundation model is trained, you can use this script to evaluate the performance of the model. \n",
    "The script will automatically load the datasets, preprocess the data, and evaluate the model on the tasks. \n",
    "The script will output the performance of the model on each task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850472a",
   "metadata": {},
   "source": [
    "## [Optional] Prepare your own benchmark datasets\n",
    "We have provided a set of benchmark datasets in the tutorials, you can use them to evaluate the performance of the model.\n",
    "If you want to evaluate the model on your own datasets, you can prepare the datasets in the following steps:\n",
    "1. Prepare the datasets in the following format:\n",
    "    - The datasets should be in the `json` format.\n",
    "    - The datasets should contain two columns: `sequence` and `label`.\n",
    "    - The `sequence` column should contain the DNA sequences.\n",
    "    - The `label` column should contain the labels of the sequences.\n",
    "2. Save the datasets in a folder like the existing benchmark datasets. This folder is referred to as the `root` in the script.\n",
    "3. Place the model and tokenizer in an accessible folder.\n",
    "4. Sometimes the tokenizer does not work well with the datasets, you can write a custom tokenizer and model wrapper in the `omnigenome_wrapper.py` file.\n",
    "More detailed documentation on how to write the custom tokenizer and model wrapper will be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c8265",
   "metadata": {},
   "source": [
    "## Prepare the benchmark environment\n",
    "Before running the benchmark, you need to install the following required packages in addition to PyTorch and other dependencies.\n",
    "Find the installation instructions for PyTorch at https://pytorch.org/get-started/locally/.\n",
    "```bash\n",
    "pip install omnigenome, findfile, autocuda, metric-visualizer, transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b5309",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "id": "201378c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T11:35:06.986777Z",
     "start_time": "2024-10-03T11:35:06.851217Z"
    }
   },
   "source": [
    "from omnigenome import AutoBench\n",
    "import autocuda"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01momnigenome\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoBench\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mautocuda\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - University of Exeter\\AIProjects\\OmniGenomeBench\\omnigenome\\__init__.py:17\u001B[0m\n\u001B[0;32m     13\u001B[0m __email__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myangheng2021@gmail.com\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     14\u001B[0m __license__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMIT\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbench\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto_bench\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto_bench\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoBench\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbench\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto_bench\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mauto_bench_config\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoBenchConfig\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbench\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbench_hub\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbench_hub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BenchHub\n",
      "File \u001B[1;32m~\\OneDrive - University of Exeter\\AIProjects\\OmniGenomeBench\\omnigenome\\bench\\auto_bench\\auto_bench.py:18\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmetric_visualizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MetricVisualizer\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TrainingArguments, Trainer \u001B[38;5;28;01mas\u001B[39;00m HFTrainer\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabstract_tokenizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OmniGenomeTokenizer\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m seed_everything, fprint, load_module_from_path\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'transformers'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "71fecad8",
   "metadata": {},
   "source": [
    "## 1. Define the root folder of the benchmark datasets\n",
    "Define the root where the benchmark datasets are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d88847",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'RGB'  # Abbreviation of the RNA genome benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e50f4c",
   "metadata": {},
   "source": [
    "## 2. Define the model and tokenizer paths\n",
    "Provide the path to the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c85e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'anonymous8/OmniGenome-52M'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f081b38bc09e5e53"
  },
  {
   "cell_type": "markdown",
   "id": "ce31d674",
   "metadata": {},
   "source": [
    "## 3. Initialize the AutoBench\n",
    "Select the available CUDA device based on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "348811f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-03 12:17:28] (0.1.1alpha) Benchmark: RGB does not exist. Search online for available benchmarks.\n",
      "[2024-10-03 12:17:28] (0.1.1alpha) Loaded benchmarks:  ['RNA-mRNA', 'RNA-SNMD', 'RNA-SNMR', 'RNA-SSP-Archive2', 'RNA-SSP-rnastralign', 'RNA-SSP-bpRNA', 'RNA-SSP-bpRNA-2000-90']\n",
      "[2024-10-03 12:17:28] (0.1.1alpha) Benchmark Root: __OMNIGENOME_DATA__/benchmarks/RGB\n",
      "Benchmark List: ['RNA-mRNA', 'RNA-SNMD', 'RNA-SNMR', 'RNA-SSP-Archive2', 'RNA-SSP-rnastralign', 'RNA-SSP-bpRNA', 'RNA-SSP-bpRNA-2000-90']\n",
      "Model Name or Path: anonymous8/OmniGenome-52M\n",
      "Tokenizer: None\n",
      "Device: cuda\n",
      "Metric Visualizer Path: __OMNIGENOME_DATA__-benchmarks-RGB-anonymous8-OmniGenome-52M.mv\n",
      "BenchConfig Details: <module 'bench_metadata' from '__OMNIGENOME_DATA__/benchmarks/RGB/metadata.py'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = autocuda.auto_cuda()\n",
    "auto_bench = AutoBench(\n",
    "    bench_root=root,\n",
    "    model_name_or_path=model_name_or_path,\n",
    "    device=\"cuda\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd396f5b",
   "metadata": {},
   "source": [
    "## 4. Run the benchmark\n",
    "The downstream tasks have predefined configurations for fair comparison.\n",
    "However, sometimes you might need to adjust the configuration based on your dataset or resources.\n",
    "For instance, adjusting the `max_length` or batch size.\n",
    "To adjust the configuration, you can override parameters in the `AutoBenchConfig` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454fc0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FindFile Warning --> multiple targets ['__OMNIGENOME_DATA__/benchmarks/RGB\\\\RNA-mRNA\\\\config.py', '__OMNIGENOME_DATA__/benchmarks/RGB\\\\RNA-mRNA\\\\__pycache__\\\\config.cpython-39.pyc'] found, only return the shortest path: <\u001B[33m__OMNIGENOME_DATA__/benchmarks/RGB\\RNA-mRNA\\config.py\u001B[0m>\n",
      "[2024-10-03 12:17:28] (0.1.1alpha) Override epochs with 10 according to the input kwargs\n",
      "[2024-10-03 12:17:28] (0.1.1alpha) Override batch_size with 8 according to the input kwargs\n",
      "[2024-10-03 12:17:28] (0.1.1alpha) Override seeds with [42, 43, 44] according to the input kwargs\n",
      "[2024-10-03 12:17:28] (0.1.1alpha) AutoBench Config for RNA-mRNA: task_name: RNA-mRNA\n",
      "task_type: token_regression\n",
      "label2id: None\n",
      "num_labels: 3\n",
      "epochs: 10\n",
      "learning_rate: 2e-05\n",
      "weight_decay: 1e-05\n",
      "batch_size: 8\n",
      "max_length: 110\n",
      "seeds: [42, 43, 44]\n",
      "compute_metrics: <function compute_metrics at 0x0000023D3692C550>\n",
      "train_file: __OMNIGENOME_DATA__/benchmarks/RGB\\RNA-mRNA/train.json\n",
      "test_file: __OMNIGENOME_DATA__/benchmarks/RGB\\RNA-mRNA/test.json\n",
      "valid_file: None\n",
      "dataset_cls: <class 'config.Dataset'>\n",
      "model_cls: <class 'omnigenome.src.model.regression.model.OmniGenomeModelForTokenRegression'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuan\\miniconda3\\lib\\site-packages\\omnigenome\\src\\abc\\abstract_tokenizer.py:39: UserWarning: No tokenizer wrapper found in anonymous8/OmniGenome-52M/omnigenome_wrapper.py -> Exception: Cannot find the module omnigenome_wrapper from anonymous8/OmniGenome-52M/omnigenome_wrapper.py.\n",
      "  warnings.warn(\n",
      "You are using a model of type omnigenome to instantiate a model of type mprna. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of OmniGenomeModel were not initialized from the model checkpoint at anonymous8/OmniGenome-52M and are newly initialized: ['OmniGenome.pooler.dense.bias', 'OmniGenome.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-03 12:17:32] (0.1.1alpha) Model Name: OmniGenomeModelForTokenRegression\n",
      "Model Metadata: {'library_name': 'OmniGenome', 'omnigenome_version': '0.1.1alpha', 'torch_version': '2.4.1+cpu+cuNone+git38b96d3399a695e704ed39b60dac733c3fbf20e2', 'transformers_version': '4.45.1', 'model_cls': 'OmniGenomeModelForTokenRegression', 'tokenizer_cls': 'EsmTokenizer', 'model_name': 'OmniGenomeModelForTokenRegression'}\n",
      "Base Model Name: anonymous8/OmniGenome-52M\n",
      "Model Type: omnigenome\n",
      "Model Architecture: None\n",
      "Model Parameters: 52.453345 M\n",
      "Model Config: OmniGenomeConfig {\n",
      "  \"OmniGenomefold_config\": null,\n",
      "  \"_name_or_path\": \"anonymous8/OmniGenome-52M\",\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"anonymous8/OmniGenome-52M--configuration_omnigenome.OmniGenomeConfig\",\n",
      "    \"AutoModel\": \"anonymous8/OmniGenome-52M--modeling_omnigenome.OmniGenomeModel\",\n",
      "    \"AutoModelForMaskedLM\": \"anonymous8/OmniGenome-52M--modeling_omnigenome.OmniGenomeForMaskedLM\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"anonymous8/OmniGenome-52M--modeling_omnigenome.OmniGenomeForSeq2SeqLM\",\n",
      "    \"AutoModelForSequenceClassification\": \"anonymous8/OmniGenome-52M--modeling_omnigenome.OmniGenomeForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"anonymous8/OmniGenome-52M--modeling_omnigenome.OmniGenomeForTokenClassification\"\n",
      "  },\n",
      "  \"classifier_dropout\": null,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0,\n",
      "  \"hidden_size\": 480,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"(\",\n",
      "    \"1\": \")\",\n",
      "    \"2\": \".\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2400,\n",
      "  \"is_folding_model\": false,\n",
      "  \"label2id\": null,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mask_token_id\": 23,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"mprna\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_generation\": 50,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_population\": 100,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"verify_ss\": true,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 24\n",
      "}\n",
      "\n",
      "\n",
      "[2024-10-03 12:17:32] (0.1.1alpha) Detected max_length=108 in the dataset, using it as the max_length.\n",
      "[2024-10-03 12:17:32] (0.1.1alpha) Loading data from __OMNIGENOME_DATA__/benchmarks/RGB\\RNA-mRNA/train.json...\n",
      "[2024-10-03 12:17:32] (0.1.1alpha) Loaded 1728 examples from __OMNIGENOME_DATA__/benchmarks/RGB\\RNA-mRNA/train.json\n",
      "[2024-10-03 12:17:32] (0.1.1alpha) Detected shuffle=True, shuffling the examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1728/1728 [00:00<00:00, 3097.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-03 12:17:33] (0.1.1alpha) {'avg_seq_len': 109.0, 'max_seq_len': 109, 'min_seq_len': 109, 'avg_label_len': 112.0, 'max_label_len': 112, 'min_label_len': 112}\n",
      "[2024-10-03 12:17:33] (0.1.1alpha) Preview of the first two samples in the dataset:\n",
      "{'input_ids': tensor([0, 6, 6, 4, 4, 4, 4, 9, 4, 4, 9, 9, 4, 5, 5, 6, 9, 6, 5, 5, 9, 5, 5, 4,\n",
      "        5, 6, 4, 4, 4, 6, 9, 4, 6, 6, 6, 4, 5, 6, 5, 5, 4, 4, 9, 5, 9, 5, 5, 4,\n",
      "        9, 6, 6, 5, 6, 6, 4, 4, 6, 5, 5, 9, 6, 4, 5, 6, 6, 9, 9, 4, 4, 6, 5, 4,\n",
      "        9, 6, 4, 6, 9, 9, 5, 6, 5, 9, 5, 4, 9, 6, 5, 4, 4, 4, 4, 6, 4, 4, 4, 5,\n",
      "        4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 2, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]), 'labels': tensor([[ 3.2630e-01,  2.3970e-01,  3.2620e-01],\n",
      "        [ 1.1321e+00,  2.2392e+00,  1.8816e+00],\n",
      "        [ 1.5661e+00,  1.0323e+00,  2.0296e+00],\n",
      "        [ 1.4618e+00,  1.0172e+00,  1.3011e+00],\n",
      "        [ 1.0462e+00,  1.1377e+00,  1.0372e+00],\n",
      "        [ 2.1425e+00,  1.6494e+00,  2.0762e+00],\n",
      "        [ 1.6369e+00,  3.7634e+00,  2.8956e+00],\n",
      "        [ 1.3015e+00,  9.6770e-01,  1.2184e+00],\n",
      "        [ 1.0998e+00,  2.6416e+00,  2.9895e+00],\n",
      "        [ 7.9980e-01,  5.1540e-01,  3.1180e-01],\n",
      "        [ 9.9050e-01,  9.3530e-01,  6.1890e-01],\n",
      "        [ 3.3470e-01,  5.8290e-01,  4.6540e-01],\n",
      "        [ 5.8100e-02,  1.5860e-01,  1.3990e-01],\n",
      "        [ 4.0600e-02,  1.3210e-01,  1.5990e-01],\n",
      "        [ 8.1500e-02,  2.6800e-01,  1.6590e-01],\n",
      "        [ 1.1440e-01,  1.1540e-01,  1.2260e-01],\n",
      "        [ 4.5720e-01,  2.5660e-01,  2.2420e-01],\n",
      "        [ 1.4700e-02,  1.0760e-01,  1.2650e-01],\n",
      "        [ 4.3730e-01,  3.3510e-01,  4.4240e-01],\n",
      "        [ 2.5370e-01,  2.7200e-02,  1.3030e-01],\n",
      "        [ 3.7300e-02,  9.3700e-02,  2.5280e-01],\n",
      "        [ 2.8070e-01,  2.2370e-01,  3.6240e-01],\n",
      "        [ 1.5120e-01,  2.1150e-01,  1.6910e-01],\n",
      "        [ 1.0490e-01,  2.4100e-01,  2.8840e-01],\n",
      "        [ 8.2500e-01,  8.1870e-01,  6.6950e-01],\n",
      "        [ 1.0446e+00,  7.2280e-01,  8.0820e-01],\n",
      "        [ 4.9550e-01,  4.1710e-01,  5.5470e-01],\n",
      "        [ 8.9480e-01,  6.0210e-01,  3.9930e-01],\n",
      "        [ 6.4510e-01,  1.5076e+00,  9.7660e-01],\n",
      "        [ 5.7040e-01,  2.5980e-01,  4.5550e-01],\n",
      "        [ 6.2200e-02,  4.0400e-02,  6.9300e-02],\n",
      "        [ 3.6100e-02,  2.3090e-01,  1.3050e-01],\n",
      "        [ 7.5700e-02,  1.2660e-01,  1.3670e-01],\n",
      "        [ 2.8800e-01,  4.0330e-01,  2.9320e-01],\n",
      "        [ 1.3020e-01,  3.0660e-01,  1.1650e-01],\n",
      "        [ 3.8100e-02,  9.9500e-02,  8.9500e-02],\n",
      "        [ 8.1900e-02,  1.8830e-01,  1.0500e-02],\n",
      "        [-1.4800e-02,  1.4590e-01,  7.3000e-03],\n",
      "        [ 8.7000e-03,  2.6960e-01,  2.1270e-01],\n",
      "        [ 3.5250e-01,  4.3150e-01,  3.5650e-01],\n",
      "        [ 1.7514e+00,  1.2023e+00,  1.1414e+00],\n",
      "        [ 2.1660e-01,  2.0070e-01,  1.9790e-01],\n",
      "        [ 5.0930e-01,  6.8050e-01,  7.3120e-01],\n",
      "        [ 4.4180e-01,  4.8430e-01,  5.5360e-01],\n",
      "        [ 9.3400e-02,  4.3880e-01,  6.3690e-01],\n",
      "        [ 2.6600e-01,  4.8380e-01,  3.6260e-01],\n",
      "        [ 3.5860e-01,  7.5980e-01,  6.9870e-01],\n",
      "        [ 5.1200e-02,  1.3100e-01,  6.4700e-02],\n",
      "        [ 2.7000e-02,  2.2410e-01,  6.6800e-02],\n",
      "        [ 6.9000e-03,  6.5700e-02,  4.8500e-02],\n",
      "        [-1.6600e-02,  5.3200e-02,  1.2500e-02],\n",
      "        [ 1.2400e-02,  9.2200e-02,  6.8500e-02],\n",
      "        [ 5.2320e-01,  7.7390e-01,  9.6540e-01],\n",
      "        [ 6.1710e-01,  6.4470e-01,  1.0076e+00],\n",
      "        [ 4.9080e-01,  2.6400e-01,  1.9450e-01],\n",
      "        [ 5.3300e-01,  2.6050e-01,  2.8890e-01],\n",
      "        [ 5.0100e-02,  9.0000e-02,  1.1630e-01],\n",
      "        [ 5.2200e-02,  2.0330e-01,  3.4810e-01],\n",
      "        [ 1.5450e-01,  6.3200e-02,  2.0800e-02],\n",
      "        [ 4.4850e-01,  3.5290e-01,  3.1120e-01],\n",
      "        [ 1.1230e-01,  1.4330e-01,  1.0750e-01],\n",
      "        [ 3.1600e-02,  1.2630e-01,  9.3700e-02],\n",
      "        [ 8.9300e-02,  3.8730e-01,  1.9210e-01],\n",
      "        [ 2.5920e-01,  1.8314e+00,  1.3322e+00],\n",
      "        [ 2.9410e-01,  1.4359e+00,  1.0734e+00],\n",
      "        [ 5.9430e-01,  5.8220e-01,  7.6650e-01],\n",
      "        [ 4.2140e-01,  7.1010e-01,  5.5010e-01],\n",
      "        [ 3.7030e-01,  7.4120e-01,  3.7880e-01],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02]])}\n",
      "{'input_ids': tensor([0, 6, 6, 4, 4, 4, 5, 6, 6, 5, 9, 4, 6, 6, 5, 6, 4, 6, 6, 5, 4, 9, 4, 6,\n",
      "        6, 5, 6, 6, 5, 4, 6, 9, 6, 6, 6, 4, 6, 6, 9, 6, 5, 6, 9, 6, 5, 4, 9, 5,\n",
      "        6, 5, 4, 5, 9, 5, 4, 5, 4, 4, 5, 9, 5, 6, 5, 9, 6, 5, 4, 9, 4, 5, 4, 6,\n",
      "        5, 5, 9, 9, 9, 9, 5, 6, 4, 4, 6, 6, 5, 9, 6, 4, 4, 4, 4, 6, 4, 4, 4, 5,\n",
      "        4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 2, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]), 'labels': tensor([[ 5.1350e-01,  2.3210e-01,  5.0290e-01],\n",
      "        [ 1.5078e+00,  2.4418e+00,  2.3320e+00],\n",
      "        [ 8.8270e-01,  8.4520e-01,  1.2860e+00],\n",
      "        [ 1.0481e+00,  5.2400e-01,  1.2027e+00],\n",
      "        [ 2.2870e-01,  7.8450e-01,  1.1479e+00],\n",
      "        [ 1.2460e-01,  1.2230e-01,  4.2740e-01],\n",
      "        [ 1.9090e-01,  2.4880e-01,  3.4830e-01],\n",
      "        [ 2.3430e-01,  4.0690e-01,  1.8000e-01],\n",
      "        [ 1.3300e-01,  9.6520e-01,  4.9360e-01],\n",
      "        [ 3.9150e-01,  5.8300e-02,  3.1320e-01],\n",
      "        [ 2.9190e-01,  1.2940e-01,  3.0550e-01],\n",
      "        [ 6.0070e-01,  7.3840e-01,  5.5870e-01],\n",
      "        [ 3.0000e-01,  5.0050e-01,  4.1620e-01],\n",
      "        [ 9.4300e-02,  4.0080e-01,  2.1310e-01],\n",
      "        [ 6.7670e-01,  5.4230e-01,  5.7080e-01],\n",
      "        [ 1.3160e-01,  1.4640e-01,  3.5930e-01],\n",
      "        [ 5.8910e-01,  5.2890e-01,  3.2760e-01],\n",
      "        [ 3.3770e-01,  7.3990e-01,  6.0060e-01],\n",
      "        [ 3.2870e-01,  5.9820e-01,  5.9880e-01],\n",
      "        [ 1.3200e+00,  2.2901e+00,  2.3748e+00],\n",
      "        [ 6.7820e-01,  4.4340e-01,  3.4720e-01],\n",
      "        [ 2.0750e-01,  2.9780e-01,  5.8980e-01],\n",
      "        [ 3.2950e-01,  5.3550e-01,  4.8200e-01],\n",
      "        [ 2.0700e-01,  4.5420e-01,  2.3280e-01],\n",
      "        [ 5.8700e-02,  4.1380e-01,  1.8920e-01],\n",
      "        [ 4.3810e-01,  5.2540e-01,  3.8950e-01],\n",
      "        [ 3.4100e-02,  7.9900e-02,  1.0410e-01],\n",
      "        [ 4.6000e-02,  2.3210e-01,  7.0700e-02],\n",
      "        [ 1.3580e-01,  2.1150e-01,  8.9200e-02],\n",
      "        [ 4.5500e-02,  2.2510e-01,  9.3400e-02],\n",
      "        [ 0.0000e+00,  8.3900e-02,  0.0000e+00],\n",
      "        [ 4.5400e-02,  5.5700e-02,  0.0000e+00],\n",
      "        [ 1.8000e-01,  2.4660e-01,  6.9800e-02],\n",
      "        [ 5.2630e-01,  3.2160e-01,  1.1580e-01],\n",
      "        [ 1.3070e-01,  5.3400e-02,  9.2300e-02],\n",
      "        [ 2.1560e-01,  7.9700e-02,  6.9000e-02],\n",
      "        [ 1.8040e-01,  2.0100e-01,  5.4900e-02],\n",
      "        [ 6.3900e-02,  5.2200e-02,  4.5800e-02],\n",
      "        [ 7.2900e-02,  4.3500e-02,  1.0000e-01],\n",
      "        [ 2.1200e-02,  2.0510e-01,  9.0900e-02],\n",
      "        [ 1.7700e-01,  1.4370e-01,  1.4400e-01],\n",
      "        [ 5.5170e-01,  2.2490e-01,  2.0140e-01],\n",
      "        [ 5.4040e-01,  2.9240e-01,  1.3590e-01],\n",
      "        [ 4.1140e-01,  4.4790e-01,  3.5160e-01],\n",
      "        [ 1.5559e+00,  9.4520e-01,  7.5470e-01],\n",
      "        [ 3.1180e-01,  4.8520e-01,  1.4020e-01],\n",
      "        [ 3.9680e-01,  1.2616e+00,  6.7190e-01],\n",
      "        [ 5.1700e-02,  1.7340e-01,  2.0400e-02],\n",
      "        [ 0.0000e+00,  1.5260e-01,  0.0000e+00],\n",
      "        [ 0.0000e+00,  1.1350e-01,  4.0700e-02],\n",
      "        [ 2.8000e-02,  3.1610e-01,  9.3700e-02],\n",
      "        [ 1.6390e-01,  5.6800e-02,  1.2700e-02],\n",
      "        [ 2.1730e-01,  2.4580e-01,  7.9200e-02],\n",
      "        [ 3.5220e-01,  1.8770e-01,  2.2500e-01],\n",
      "        [ 1.6590e-01,  3.3160e-01,  2.9790e-01],\n",
      "        [ 5.8050e-01,  2.3950e-01,  1.6700e-01],\n",
      "        [ 1.6560e-01,  1.3540e-01,  1.1400e-02],\n",
      "        [ 6.6700e-02,  1.7530e-01,  1.0220e-01],\n",
      "        [ 1.5700e-02,  4.9900e-02,  1.9300e-02],\n",
      "        [ 1.5560e-01,  2.1320e-01,  1.5350e-01],\n",
      "        [ 1.5500e-02,  1.4610e-01,  5.7400e-02],\n",
      "        [-7.8000e-03,  1.1210e-01,  1.0800e-02],\n",
      "        [-1.1700e-02,  1.3510e-01,  6.2500e-02],\n",
      "        [ 6.2000e-02,  1.4990e-01,  2.2650e-01],\n",
      "        [ 5.4100e-02,  4.3630e-01,  2.0590e-01],\n",
      "        [ 9.6020e-01,  1.3602e+00,  1.1152e+00],\n",
      "        [ 1.2813e+00,  2.3379e+00,  1.3669e+00],\n",
      "        [ 9.4800e-02,  2.0907e+00,  1.4542e+00],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02]])}\n",
      "[2024-10-03 12:17:33] (0.1.1alpha) Detected max_length=108 in the dataset, using it as the max_length.\n",
      "[2024-10-03 12:17:33] (0.1.1alpha) Loading data from __OMNIGENOME_DATA__/benchmarks/RGB\\RNA-mRNA/test.json...\n",
      "[2024-10-03 12:17:33] (0.1.1alpha) Loaded 192 examples from __OMNIGENOME_DATA__/benchmarks/RGB\\RNA-mRNA/test.json\n",
      "[2024-10-03 12:17:33] (0.1.1alpha) Detected shuffle=True, shuffling the examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 192/192 [00:00<00:00, 2988.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-03 12:17:33] (0.1.1alpha) {'avg_seq_len': 109.0, 'max_seq_len': 109, 'min_seq_len': 109, 'avg_label_len': 112.0, 'max_label_len': 112, 'min_label_len': 112}\n",
      "[2024-10-03 12:17:33] (0.1.1alpha) Preview of the first two samples in the dataset:\n",
      "{'input_ids': tensor([0, 6, 6, 4, 4, 4, 4, 9, 9, 5, 4, 4, 4, 5, 5, 4, 9, 6, 5, 9, 9, 5, 4, 6,\n",
      "        9, 5, 4, 6, 5, 9, 6, 4, 9, 6, 5, 4, 5, 4, 4, 9, 5, 6, 9, 9, 9, 9, 9, 4,\n",
      "        4, 4, 5, 6, 6, 4, 9, 9, 9, 4, 4, 9, 4, 4, 9, 4, 4, 9, 4, 4, 4, 6, 5, 6,\n",
      "        6, 9, 5, 6, 9, 9, 5, 6, 5, 6, 6, 9, 5, 6, 5, 4, 4, 4, 4, 6, 4, 4, 4, 5,\n",
      "        4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 2, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]), 'labels': tensor([[ 1.0480e-01,  3.6290e-01,  3.9840e-01],\n",
      "        [ 9.1180e-01,  2.4665e+00,  2.1918e+00],\n",
      "        [ 1.7566e+00,  1.3311e+00,  1.3399e+00],\n",
      "        [ 1.0462e+00,  3.0080e-01,  4.8740e-01],\n",
      "        [ 1.3930e+00,  6.6640e-01,  9.8070e-01],\n",
      "        [ 1.7828e+00,  1.5078e+00,  1.4385e+00],\n",
      "        [ 2.0306e+00,  2.0011e+00,  1.8866e+00],\n",
      "        [ 9.9520e-01,  1.0072e+00,  1.0956e+00],\n",
      "        [ 6.5280e-01,  1.4352e+00,  1.9469e+00],\n",
      "        [ 9.7960e-01,  6.6870e-01,  1.0335e+00],\n",
      "        [ 1.0039e+00,  3.8920e-01,  5.6650e-01],\n",
      "        [ 6.6130e-01,  3.1120e-01,  5.7780e-01],\n",
      "        [ 6.2000e-02,  4.4700e-01,  7.3000e-01],\n",
      "        [ 6.8800e-02,  1.8550e-01,  2.9010e-01],\n",
      "        [ 2.4920e-01,  2.7270e-01,  5.9710e-01],\n",
      "        [ 1.0230e-01,  1.8820e-01,  1.7080e-01],\n",
      "        [ 2.6700e-02,  2.8390e-01,  1.3960e-01],\n",
      "        [ 8.4800e-02,  4.2340e-01,  2.6920e-01],\n",
      "        [ 2.8600e-02,  1.8210e-01,  7.0400e-02],\n",
      "        [ 4.7500e-02,  8.3800e-02,  7.6600e-02],\n",
      "        [ 4.2000e-03,  2.8900e-01,  1.1400e-01],\n",
      "        [ 1.1280e-01,  1.4080e-01,  1.1860e-01],\n",
      "        [ 2.4730e-01,  7.4790e-01,  3.2900e-01],\n",
      "        [ 6.8080e-01,  1.0090e-01,  1.6500e-02],\n",
      "        [ 3.2060e-01,  2.2930e-01,  1.7230e-01],\n",
      "        [ 2.0410e-01,  2.8010e-01,  1.8340e-01],\n",
      "        [ 9.5650e-01,  3.8230e-01,  2.1610e-01],\n",
      "        [ 5.4900e-02,  3.2110e-01,  1.7630e-01],\n",
      "        [-1.1000e-02,  5.5800e-02,  8.6000e-03],\n",
      "        [ 4.6600e-02,  2.1170e-01,  7.8200e-02],\n",
      "        [ 9.9300e-02,  2.4760e-01,  1.3490e-01],\n",
      "        [ 5.3500e-02,  1.3350e-01,  7.3800e-02],\n",
      "        [-2.6060e-01, -1.8880e-01, -6.1370e-01],\n",
      "        [-2.3690e-01, -6.8200e-01, -1.5855e+00],\n",
      "        [ 3.8260e-01,  2.0860e-01,  1.2950e-01],\n",
      "        [ 2.0710e-01,  5.5370e-01,  9.5370e-01],\n",
      "        [ 3.1660e-01,  2.3940e-01,  3.4510e-01],\n",
      "        [ 9.3180e-01,  5.3880e-01,  5.9050e-01],\n",
      "        [ 1.8010e-01,  1.2280e-01,  2.1580e-01],\n",
      "        [ 1.7800e-01,  1.7200e-01,  1.1540e-01],\n",
      "        [ 2.4190e-01,  6.9040e-01,  3.3270e-01],\n",
      "        [ 2.4610e-01,  5.9750e-01,  6.1260e-01],\n",
      "        [ 4.1300e-01,  8.6870e-01,  5.9460e-01],\n",
      "        [ 1.5274e+00,  3.0490e-01,  2.6510e-01],\n",
      "        [ 9.2170e-01,  1.2966e+00,  1.2654e+00],\n",
      "        [ 7.0040e-01,  5.2030e-01,  5.8460e-01],\n",
      "        [ 6.0960e-01,  2.8410e-01,  5.8800e-01],\n",
      "        [ 3.6780e-01,  1.9570e-01,  2.2170e-01],\n",
      "        [ 4.5800e-02,  1.4540e-01,  1.7900e-01],\n",
      "        [ 1.8600e-02,  6.9100e-02,  6.6100e-02],\n",
      "        [ 9.3300e-02,  1.2070e-01,  2.5000e-02],\n",
      "        [ 5.3770e-01,  3.9130e-01,  3.2780e-01],\n",
      "        [ 8.1190e-01,  1.6598e+00,  1.5675e+00],\n",
      "        [ 1.1420e+00,  1.3687e+00,  1.3894e+00],\n",
      "        [ 4.6380e-01,  1.2568e+00,  1.5601e+00],\n",
      "        [ 5.3500e-02,  6.7080e-01,  4.2460e-01],\n",
      "        [ 1.8870e-01,  1.2290e-01,  1.6110e-01],\n",
      "        [ 8.9110e-01,  7.5700e-01,  1.0612e+00],\n",
      "        [ 3.9420e-01,  1.2667e+00,  1.2074e+00],\n",
      "        [ 3.6030e-01,  5.3540e-01,  4.0550e-01],\n",
      "        [ 1.1861e+00,  1.2808e+00,  1.2162e+00],\n",
      "        [ 8.6760e-01,  1.8105e+00,  1.3085e+00],\n",
      "        [ 5.4000e-01,  4.4160e-01,  3.9580e-01],\n",
      "        [ 8.3210e-01,  1.0980e+00,  1.0872e+00],\n",
      "        [ 7.7270e-01,  1.7774e+00,  1.2667e+00],\n",
      "        [ 5.4170e-01,  6.7280e-01,  7.8040e-01],\n",
      "        [ 3.5820e-01,  4.3440e-01,  5.0280e-01],\n",
      "        [ 3.9860e-01,  5.3500e-01,  3.7470e-01],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02]])}\n",
      "{'input_ids': tensor([0, 6, 6, 4, 4, 4, 9, 4, 4, 4, 9, 4, 4, 4, 9, 4, 4, 5, 4, 4, 9, 4, 4, 4,\n",
      "        6, 4, 6, 4, 9, 4, 4, 6, 4, 5, 4, 5, 4, 4, 9, 4, 4, 4, 9, 4, 4, 4, 4, 9,\n",
      "        4, 4, 9, 4, 4, 9, 6, 4, 4, 9, 4, 4, 4, 9, 4, 4, 4, 9, 4, 4, 4, 6, 6, 9,\n",
      "        4, 9, 6, 5, 9, 9, 5, 6, 6, 5, 6, 9, 6, 5, 5, 4, 4, 4, 4, 6, 4, 4, 4, 5,\n",
      "        4, 4, 5, 4, 4, 5, 4, 4, 5, 4, 4, 5, 2, 1, 1, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]), 'labels': tensor([[ 2.3220e-01,  3.6690e-01,  3.2560e-01],\n",
      "        [ 1.0826e+00,  9.8580e-01,  7.4640e-01],\n",
      "        [ 6.6750e-01,  1.8580e-01,  1.7770e-01],\n",
      "        [ 8.5990e-01,  1.7170e-01,  1.7860e-01],\n",
      "        [ 6.7150e-01,  1.6440e-01,  1.8650e-01],\n",
      "        [ 2.3880e-01,  9.2480e-01,  5.0240e-01],\n",
      "        [ 4.5470e-01,  1.6400e-01,  2.3680e-01],\n",
      "        [ 2.1510e-01,  4.0040e-01,  3.1420e-01],\n",
      "        [ 4.7940e-01,  1.2100e-01,  2.2010e-01],\n",
      "        [ 4.6720e-01,  4.7910e-01,  6.7210e-01],\n",
      "        [ 4.2470e-01,  2.4410e-01,  9.8800e-02],\n",
      "        [ 6.3600e-01,  3.0930e-01,  2.2720e-01],\n",
      "        [ 4.4820e-01,  3.6520e-01,  1.6910e-01],\n",
      "        [ 4.3740e-01,  2.0050e-01,  2.0400e-01],\n",
      "        [ 8.7040e-01,  1.9540e-01,  7.5400e-02],\n",
      "        [ 3.4590e-01,  1.8060e-01,  9.2900e-02],\n",
      "        [ 3.1530e-01,  1.9200e-02, -5.8260e-01],\n",
      "        [ 6.6870e-01,  1.7530e-01, -3.0600e-02],\n",
      "        [ 7.4720e-01,  3.6320e-01, -6.9000e-03],\n",
      "        [ 5.9500e-01,  5.9130e-01,  1.4640e-01],\n",
      "        [ 8.8800e-01,  3.6760e-01,  2.0800e-02],\n",
      "        [ 5.3330e-01,  2.2650e-01,  7.1600e-02],\n",
      "        [ 6.8200e-02,  3.0800e-02, -1.0690e-01],\n",
      "        [ 1.9200e-02, -2.5386e+00, -4.5436e+00],\n",
      "        [ 9.0900e-02, -3.5430e-01, -7.1790e-01],\n",
      "        [ 3.5300e-01, -2.2510e-01, -7.5970e-01],\n",
      "        [ 7.2190e-01, -7.5400e-02, -3.4500e-01],\n",
      "        [ 3.5530e-01, -8.9000e-03, -3.5360e-01],\n",
      "        [ 6.8300e-02,  2.4970e-01,  1.9700e-02],\n",
      "        [ 1.5700e-02,  5.4700e-02, -2.4500e-02],\n",
      "        [ 4.6190e-01,  3.0260e-01,  9.8000e-03],\n",
      "        [ 2.6490e-01,  9.7800e-02, -6.1000e-02],\n",
      "        [ 3.0260e-01,  1.9000e-01, -7.6200e-02],\n",
      "        [ 1.7990e-01,  1.4440e-01,  2.8000e-03],\n",
      "        [ 2.8140e-01,  1.6210e-01,  1.5300e-02],\n",
      "        [ 7.5300e-02,  1.3980e-01,  1.9600e-02],\n",
      "        [ 3.3240e-01,  2.4190e-01,  1.5560e-01],\n",
      "        [ 2.8270e-01,  2.9210e-01,  8.0200e-02],\n",
      "        [ 3.3140e-01,  1.9770e-01,  1.2930e-01],\n",
      "        [ 2.7440e-01,  6.4900e-02,  7.4800e-02],\n",
      "        [ 3.5020e-01,  1.9340e-01,  1.2760e-01],\n",
      "        [ 2.1040e-01,  3.0340e-01,  2.7070e-01],\n",
      "        [ 3.1230e-01,  1.9120e-01,  2.0020e-01],\n",
      "        [ 2.1880e-01,  1.1180e-01,  4.7600e-02],\n",
      "        [ 3.1820e-01,  2.7100e-02,  1.4190e-01],\n",
      "        [ 5.7250e-01,  3.7550e-01,  3.3940e-01],\n",
      "        [ 3.2170e-01,  3.8600e-01,  2.6550e-01],\n",
      "        [ 4.2010e-01,  2.6900e-01,  3.4470e-01],\n",
      "        [ 6.7360e-01,  4.3890e-01,  8.5500e-01],\n",
      "        [ 3.5900e-01,  7.6610e-01,  7.2220e-01],\n",
      "        [ 4.3820e-01,  2.8600e-01,  2.1420e-01],\n",
      "        [ 2.4370e-01,  2.5020e-01,  2.7250e-01],\n",
      "        [ 1.1910e-01,  2.6690e-01,  2.4020e-01],\n",
      "        [ 5.6290e-01,  6.0110e-01,  6.1470e-01],\n",
      "        [ 3.4650e-01,  3.5150e-01,  5.1470e-01],\n",
      "        [ 3.7410e-01,  5.5120e-01,  7.4570e-01],\n",
      "        [ 2.7290e-01,  1.0611e+00,  1.5944e+00],\n",
      "        [ 3.7680e-01,  3.3270e-01,  4.8910e-01],\n",
      "        [ 3.9240e-01,  4.6100e-01,  6.6230e-01],\n",
      "        [ 5.6790e-01,  9.1490e-01,  1.1789e+00],\n",
      "        [ 6.7730e-01,  1.5715e+00,  1.5917e+00],\n",
      "        [ 6.4330e-01,  7.0770e-01,  1.1081e+00],\n",
      "        [ 4.6880e-01,  3.4090e-01,  6.5280e-01],\n",
      "        [ 1.1274e+00,  1.5072e+00,  1.5333e+00],\n",
      "        [ 3.4890e-01,  1.2604e+00,  1.3857e+00],\n",
      "        [ 7.0650e-01,  1.5064e+00,  1.3996e+00],\n",
      "        [ 4.8660e-01,  4.8330e-01,  5.6580e-01],\n",
      "        [ 1.5990e-01,  2.0990e-01,  1.9620e-01],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02],\n",
      "        [-1.0000e+02, -1.0000e+02, -1.0000e+02]])}\n",
      "[2024-10-03 12:17:33] (0.1.1alpha) Detected max_length=108 in the dataset, using it as the max_length.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\chuan\\miniconda3\\lib\\site-packages\\omnigenome\\src\\trainer\\trainer.py:120: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n",
      "C:\\Users\\chuan\\miniconda3\\lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m      3\u001B[0m seeds \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m42\u001B[39m, \u001B[38;5;241m43\u001B[39m, \u001B[38;5;241m44\u001B[39m]\n\u001B[1;32m----> 4\u001B[0m \u001B[43mauto_bench\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseeds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\omnigenome\\bench\\auto_bench\\auto_bench.py:287\u001B[0m, in \u001B[0;36mAutoBench.run\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    278\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdamW(\n\u001B[0;32m    279\u001B[0m         model\u001B[38;5;241m.\u001B[39mparameters(),\n\u001B[0;32m    280\u001B[0m         lr\u001B[38;5;241m=\u001B[39mbench_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    285\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m    286\u001B[0m     )\n\u001B[1;32m--> 287\u001B[0m     trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m    288\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m    289\u001B[0m         train_dataset\u001B[38;5;241m=\u001B[39mtrain_set,\n\u001B[0;32m    290\u001B[0m         eval_dataset\u001B[38;5;241m=\u001B[39mvalid_set,\n\u001B[0;32m    291\u001B[0m         test_dataset\u001B[38;5;241m=\u001B[39mtest_set,\n\u001B[0;32m    292\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m    293\u001B[0m         patience\u001B[38;5;241m=\u001B[39mbench_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatience\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatience\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m bench_config \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m    294\u001B[0m         epochs\u001B[38;5;241m=\u001B[39mbench_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    295\u001B[0m         gradient_accumulation_steps\u001B[38;5;241m=\u001B[39mbench_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgradient_accumulation_steps\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m    296\u001B[0m         optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[0;32m    297\u001B[0m         loss_fn\u001B[38;5;241m=\u001B[39mbench_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss_fn\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss_fn\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m bench_config \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    298\u001B[0m         compute_metrics\u001B[38;5;241m=\u001B[39mbench_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompute_metrics\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    299\u001B[0m         seed\u001B[38;5;241m=\u001B[39mseed,\n\u001B[0;32m    300\u001B[0m         device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice,\n\u001B[0;32m    301\u001B[0m         autocast\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautocast,\n\u001B[0;32m    302\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_kwargs,\n\u001B[0;32m    303\u001B[0m     )\n\u001B[0;32m    305\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m    307\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\omnigenome\\src\\trainer\\trainer.py:124\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, train_dataset, eval_dataset, test_dataset, epochs, batch_size, patience, gradient_accumulation_steps, optimizer, loss_fn, compute_metrics, seed, device, autocast, **kwargs)\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    122\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mset_loss_fn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn)\n\u001B[1;32m--> 124\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m env_meta_info()\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetrics \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1171\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m-> 1174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    801\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    802\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    803\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 805\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    806\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    808\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1153\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1154\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[0;32m   1155\u001B[0m             device,\n\u001B[0;32m   1156\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1157\u001B[0m             non_blocking,\n\u001B[0;32m   1158\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[0;32m   1159\u001B[0m         )\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1163\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1164\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:305\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    300\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    301\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    303\u001B[0m     )\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    307\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    309\u001B[0m     )\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 10\n",
    "seeds = [42, 43, 44]\n",
    "auto_bench.run(epochs=epochs, batch_size=batch_size, seeds=seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f3be7",
   "metadata": {},
   "source": [
    "## 5. Benchmark Checkpointing\n",
    "Whenever the benchmark is interrupted, the results will be saved and available for further execution.\n",
    "You can also clear the checkpoint to start fresh:\n",
    "```python\n",
    "AutoBench(bench_root=root, model_name_or_path=model_name_or_path, device=device, overwrite=True).run()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
