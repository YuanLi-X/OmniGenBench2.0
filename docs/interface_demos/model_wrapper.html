
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Model Wrapper Interface</title>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
  <style>
    body {
      background-color: #1e1e1e;
      color: #ccc;
      font-family: monospace;
      padding: 2rem;
    }
    pre {
      background: #2d2d2d;
      padding: 1rem;
      border-radius: 8px;
      font-size: 14px;
      line-height: 1.5;
      overflow-x: auto;
    }
    .cursor {
      display: inline-block;
      width: 8px;
      height: 1em;
      background: #ccc;
      animation: blink 1s step-start infinite;
      vertical-align: bottom;
      margin-left: -2px;
    }
    @keyframes blink {
      50% { background: transparent; }
    }
  </style>
</head>
<body>
  <h1>Model Wrapper Interface</h1>
  <pre class="language-python"><code id="code-block"></code><span class="cursor"></span></pre>

  <!-- Prism.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

  <script>
const rawCode = `import numpy as np
import torch

from omnigenome import (
    AutoBenchConfig,
    OmniGenomeModelForTokenRegression,
    RegressionMetric,
)

class Model(OmniGenomeModelForTokenRegression):
    """
    Custom model class for token regression tasks, inherited from OmniGenomeModelForTokenRegression
    """
    def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
        super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
        self.metadata["model_name"] = self.__class__.__name__
        self.classifier = torch.nn.Linear(
            self.config.hidden_size, self.config.num_labels
        )
        self.loss_fn = torch.nn.MSELoss()
        self.model_info()

    # Forward pass of the model
    def forward(self, **inputs):
        labels = inputs.pop("labels", None)
        last_hidden_state = self.last_hidden_state_forward(**inputs)
        last_hidden_state = self.dropout(last_hidden_state)
        last_hidden_state = self.activation(last_hidden_state)
        logits = self.classifier(last_hidden_state)
        outputs = {
            "logits": logits,
            "last_hidden_state": last_hidden_state,
            "labels": labels,
        }
        return outputs

    # Prediction method for the model, outputs logits and predictions
    def predict(self, sequence_or_inputs, **kwargs):
        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)

        logits = raw_outputs["logits"]
        last_hidden_state = raw_outputs["last_hidden_state"]

        predictions = []
        for i in range(logits.shape[0]):
            predictions.append(logits[i].cpu())

        outputs = {
            "predictions": (
                torch.vstack(predictions).to(self.model.device)
                if predictions[0].shape
                else torch.tensor(predictions).to(self.model.device)
            ),
            "logits": logits,
            "last_hidden_state": last_hidden_state,
        }

        return outputs

    # Inference method for generating model predictions
    def inference(self, sequence_or_inputs, **kwargs):
        raw_outputs = self._forward_from_raw_input(sequence_or_inputs, **kwargs)

        inputs = raw_outputs["inputs"]
        logits = raw_outputs["logits"]
        last_hidden_state = raw_outputs["last_hidden_state"]

        predictions = []
        for i in range(logits.shape[0]):
            i_logit = logits[i][inputs["input_ids"][i].ne(self.config.pad_token_id)][
                      1:-1
                      ]
            predictions.append(i_logit.detach().cpu())

        if not isinstance(sequence_or_inputs, list):
            outputs = {
                "predictions": predictions[0],
                "logits": logits[0],
                "last_hidden_state": last_hidden_state[0],
            }
        else:
            outputs = {
                "predictions": predictions,
                "logits": logits,
                "last_hidden_state": last_hidden_state,
            }

        return outputs

`;


    let i = 0;
    let buffer = "";
    let throttle = 1;
    const codeBlock = document.getElementById("code-block");
    const cursor = document.querySelector(".cursor");

    function typeChar() {
      while (i < rawCode.length && rawCode[i] === ' ') {
        buffer += rawCode[i++];
      }
      if (i < rawCode.length) {
        buffer += rawCode[i++];
        if (i % throttle === 0 || i === rawCode.length) {
          codeBlock.innerHTML = Prism.highlight(buffer, Prism.languages.python, 'python');
        }
        setTimeout(typeChar, 20);
      } else {
        cursor.remove();
        codeBlock.innerHTML = Prism.highlight(buffer, Prism.languages.python, 'python');
      }
    }

    typeChar();
  </script>
</body>
</html>
